{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "706fe9b0",
   "metadata": {},
   "source": [
    "- https://pypi.org/project/langchain/\n",
    "- https://pypi.org/project/llama-index/ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394ad355",
   "metadata": {},
   "source": [
    "### `Langchain Functionaities`\n",
    "- **`Installation `**\n",
    "- **`Loading LLMs With Langchain` (`Model I/O` )**\n",
    "- **`Prompt Template`**\n",
    "- **`Chains`**\n",
    "- **`Agents & Tools`**\n",
    "- **`Memory`**\n",
    "- **`Document Loader`**\n",
    "- **`ChatModels`**\n",
    "- **`Data Connectors`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "256172f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Own\\.conda\\envs\\genai\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "#pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cac3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install python-dotenv\n",
    "# pip install openai\n",
    "# pip install langchain_community\n",
    "# pip install huggingface_hub\n",
    "# pip install langchain_google_genai\n",
    "# pip install google-search-results\n",
    "# pip install numexpr\n",
    "# pip install wikipedia\n",
    "# pip install pypdf\n",
    "\n",
    "# pip install python-dotenv openai langchain_community huggingface_hub google-search-results numexpr wikipedia pypdf -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9abdf79f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langchain\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c914507",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GOOGLE_API_KEY = os.getenv['GOOGLE_API_KEY']\n",
    "GOOGLE_API_KEY = load_dotenv('GOOGLE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c01cbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGINGFACE_KEY = load_dotenv('HUGGINGFACE_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c3bce7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = load_dotenv('OPENAI_API_KEY1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adcd3a3",
   "metadata": {},
   "source": [
    "## `Langchain with OpenAI API `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8ac080c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3bff09",
   "metadata": {},
   "source": [
    "- **`temperature`**:\n",
    " - 0 : Model is save and its not taking any bets\n",
    " - 1 : It will take risk it might generate wrong output but it is very creative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "034021b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Own\\.conda\\envs\\genai\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OpenAI(client=<openai.resources.completions.Completions object at 0x0000029CE82457F0>, async_client=<openai.resources.completions.AsyncCompletions object at 0x0000029CE83B8610>, temperature=0.9, openai_api_key='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', openai_proxy='')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_1 = OpenAI(temperature = 0.9)\n",
    "llm_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "582de7a3",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Own\\.conda\\envs\\genai\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `BaseLLM.predict` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan you tell me about the china?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mllm_1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\genai\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:148\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    146\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    147\u001b[0m     emit_warning()\n\u001b[1;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\genai\\lib\\site-packages\\langchain_core\\language_models\\llms.py:1127\u001b[0m, in \u001b[0;36mBaseLLM.predict\u001b[1;34m(self, text, stop, **kwargs)\u001b[0m\n\u001b[0;32m   1125\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1126\u001b[0m     _stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(stop)\n\u001b[1;32m-> 1127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(text, stop\u001b[38;5;241m=\u001b[39m_stop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\genai\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:148\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    146\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    147\u001b[0m     emit_warning()\n\u001b[1;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\genai\\lib\\site-packages\\langchain_core\\language_models\\llms.py:1086\u001b[0m, in \u001b[0;36mBaseLLM.__call__\u001b[1;34m(self, prompt, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[0;32m   1079\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(prompt, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   1080\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1081\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `prompt` is expected to be a string. Instead found \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1082\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(prompt)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. If you want to run the LLM on multiple prompts, use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1083\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`generate` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1084\u001b[0m     )\n\u001b[0;32m   1085\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m-> 1086\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m   1087\u001b[0m         [prompt],\n\u001b[0;32m   1088\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m   1089\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m   1090\u001b[0m         tags\u001b[38;5;241m=\u001b[39mtags,\n\u001b[0;32m   1091\u001b[0m         metadata\u001b[38;5;241m=\u001b[39mmetadata,\n\u001b[0;32m   1092\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1093\u001b[0m     )\n\u001b[0;32m   1094\u001b[0m     \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1095\u001b[0m     \u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m   1096\u001b[0m )\n",
      "File \u001b[1;32m~\\.conda\\envs\\genai\\lib\\site-packages\\langchain_core\\language_models\\llms.py:803\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    789\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    790\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    791\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    801\u001b[0m         )\n\u001b[0;32m    802\u001b[0m     ]\n\u001b[1;32m--> 803\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_helper(\n\u001b[0;32m    804\u001b[0m         prompts, stop, run_managers, \u001b[38;5;28mbool\u001b[39m(new_arg_supported), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    805\u001b[0m     )\n\u001b[0;32m    806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[0;32m    807\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\.conda\\envs\\genai\\lib\\site-packages\\langchain_core\\language_models\\llms.py:670\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    668\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[0;32m    669\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 670\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    671\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[1;32m~\\.conda\\envs\\genai\\lib\\site-packages\\langchain_core\\language_models\\llms.py:657\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    647\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[0;32m    648\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    649\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    653\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    654\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    655\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    656\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 657\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    658\u001b[0m                 prompts,\n\u001b[0;32m    659\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    660\u001b[0m                 \u001b[38;5;66;03m# TODO: support multiple run managers\u001b[39;00m\n\u001b[0;32m    661\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    662\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    663\u001b[0m             )\n\u001b[0;32m    664\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    665\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m    666\u001b[0m         )\n\u001b[0;32m    667\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    668\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[1;32m~\\.conda\\envs\\genai\\lib\\site-packages\\langchain_community\\llms\\openai.py:460\u001b[0m, in \u001b[0;36mBaseOpenAI._generate\u001b[1;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    448\u001b[0m     choices\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m    449\u001b[0m         {\n\u001b[0;32m    450\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: generation\u001b[38;5;241m.\u001b[39mtext,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    457\u001b[0m         }\n\u001b[0;32m    458\u001b[0m     )\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 460\u001b[0m     response \u001b[38;5;241m=\u001b[39m completion_with_retry(\n\u001b[0;32m    461\u001b[0m         \u001b[38;5;28mself\u001b[39m, prompt\u001b[38;5;241m=\u001b[39m_prompts, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[0;32m    462\u001b[0m     )\n\u001b[0;32m    463\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    464\u001b[0m         \u001b[38;5;66;03m# V1 client returns the response in an PyDantic object instead of\u001b[39;00m\n\u001b[0;32m    465\u001b[0m         \u001b[38;5;66;03m# dict. For the transition period, we deep convert it to dict.\u001b[39;00m\n\u001b[0;32m    466\u001b[0m         response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mdict()\n",
      "File \u001b[1;32m~\\.conda\\envs\\genai\\lib\\site-packages\\langchain_community\\llms\\openai.py:115\u001b[0m, in \u001b[0;36mcompletion_with_retry\u001b[1;34m(llm, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Use tenacity to retry the completion call.\"\"\"\u001b[39;00m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_openai_v1():\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m llm\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    117\u001b[0m retry_decorator \u001b[38;5;241m=\u001b[39m _create_retry_decorator(llm, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[0;32m    119\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_completion_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n",
      "File \u001b[1;32m~\\.conda\\envs\\genai\\lib\\site-packages\\openai\\_utils\\_utils.py:277\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\genai\\lib\\site-packages\\openai\\resources\\completions.py:528\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, model, prompt, best_of, echo, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, seed, stop, stream, stream_options, suffix, temperature, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    501\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    526\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    527\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Completion \u001b[38;5;241m|\u001b[39m Stream[Completion]:\n\u001b[1;32m--> 528\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    531\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    532\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    533\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    534\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbest_of\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    535\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mecho\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mecho\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    536\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    537\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    538\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    539\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    540\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    541\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    542\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    543\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    544\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    545\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    546\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msuffix\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    547\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    548\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    549\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    552\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    553\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    554\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    555\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    556\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    558\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mCompletion\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    559\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\genai\\lib\\site-packages\\openai\\_base_client.py:1240\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1226\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1227\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1228\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1235\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1236\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1237\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1238\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1239\u001b[0m     )\n\u001b[1;32m-> 1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\genai\\lib\\site-packages\\openai\\_base_client.py:921\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    912\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    913\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    914\u001b[0m     cast_to: Type[ResponseT],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    919\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    920\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m--> 921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\genai\\lib\\site-packages\\openai\\_base_client.py:1005\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1003\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m   1004\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m-> 1005\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1006\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1007\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1008\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1009\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1012\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1014\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32m~\\.conda\\envs\\genai\\lib\\site-packages\\openai\\_base_client.py:1053\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1049\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m   1050\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m   1051\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1054\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1055\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1056\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1057\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1058\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1059\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\genai\\lib\\site-packages\\openai\\_base_client.py:1005\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1003\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m   1004\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m-> 1005\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1006\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1007\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1008\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1009\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1012\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1014\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32m~\\.conda\\envs\\genai\\lib\\site-packages\\openai\\_base_client.py:1053\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1049\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m   1050\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m   1051\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1054\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1055\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1056\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1057\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1058\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1059\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\genai\\lib\\site-packages\\openai\\_base_client.py:1020\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1017\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1019\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1020\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m   1023\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1024\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1027\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1028\u001b[0m )\n",
      "\u001b[1;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "text = \"Can you tell me about the china?\"\n",
    "print(llm_1.predict(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6b6feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the exact answer from the model generated output.\n",
    "print(llm_1.invoke(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907bcd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the direct result instead of using 'invoke'.\n",
    "print(llm_1(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211ee8f2",
   "metadata": {},
   "source": [
    "##  `Langchain with HuggingFace Hub`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b234e0b4",
   "metadata": {},
   "source": [
    "### `Loading LLM with Langchain`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444ebbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFaceHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad91fa95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Own\\.conda\\envs\\genai\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 0.3.0. Use HuggingFaceEndpoint instead.\n",
      "  warn_deprecated(\n",
      "C:\\Users\\Own\\.conda\\envs\\genai\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# llm_2 = HuggingFaceHub(repo_id = 'google/gemma-7b', huggingfacehub_api_token = HUGGINGFACE_KEY)\n",
    "# llm_2 = HuggingFaceHub(repo_id = 'google/gemma-2b', huggingfacehub_api_token = HUGGINGFACE_KEY)\n",
    "# llm_2 = HuggingFaceHub(repo_id = 'meta-llama/Llama-2-7b-chat-hf', huggingfacehub_api_token = HUGGINGFACE_KEY)\n",
    "# llm_2 = HuggingFaceHub(repo_id = 'mistralai/Mistral-7B-Instruct-v0.2', huggingfacehub_api_token = HUGGINGFACE_KEY)\n",
    "llm_2 = HuggingFaceHub(repo_id = 'google/flan-t5-large', huggingfacehub_api_token = HUGGINGFACE_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b05307fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Own\\.conda\\envs\\genai\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'India is a country in the Indian subcontinent.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_2('Please write couple of lines of description on India. I want a complete paragraph')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1ada138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is Text Generation Model.\n",
    "llm_t5 = HuggingFaceHub(repo_id = 'google/flan-t5-large', model_kwargs = {'temperature': 0.8}, huggingfacehub_api_token = HUGGINGFACE_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e44e4e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wie sind Sie?\n"
     ]
    }
   ],
   "source": [
    "text = 'translate englist to German : How are you?'\n",
    "print(llm_t5.invoke(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed0d6911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wie sind Sie?\n"
     ]
    }
   ],
   "source": [
    "print(llm_t5.predict(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6258ffda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting in elections is a good way to make sure that your voice is heard.\n"
     ]
    }
   ],
   "source": [
    "text = 'Summarize the importance of Voting in an election: provide me a paragrapgh'\n",
    "print(llm_t5.invoke(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ca2a059",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_fa = HuggingFaceHub(repo_id = 'tiiuae/falcon-7b', model_kwargs = {'temperature': 0.6}, huggingfacehub_api_token = HUGGINGFACE_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb9ef34c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "translate english to German : 'How are you?'\n",
      "Budjetti $10-30 USD\n",
      "I have to translate a phrase of 20 words from English to German.\n",
      "Myönnetty käyttäjälle:\n",
      "14 freelanceria on tarjonnut keskimäärin %project_bid_stats_avg_sub_26% %project_currencyDetails_sign_sub_27% tähän työhön\n",
      "Good day. I am a native German and I have\n"
     ]
    }
   ],
   "source": [
    "text = \"translate english to German : 'How are you?'\"\n",
    "print(llm_fa.invoke(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d28e8715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "translate english to German : 'How are you?'\n",
      "Budjetti $10-30 USD\n",
      "I have to translate a phrase of 20 words from English to German.\n",
      "Myönnetty käyttäjälle:\n",
      "14 freelanceria on tarjonnut keskimäärin %project_bid_stats_avg_sub_26% %project_currencyDetails_sign_sub_27% tähän työhön\n",
      "Good day. I am a native German and I have\n"
     ]
    }
   ],
   "source": [
    "print(llm_fa.predict(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4b54ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize the importance of Voting in an election: provide me a paragrapgh\n",
      "Summarize the importance of Voting in an election: provide me a paragrapgh\n",
      "Summarize the importance of Voting in an election: provide me a paragrapgh\n",
      "Summarize the importance of Voting in an election: provide me a paragrapgh\n",
      "Summarize the importance of Voting in an election: provide me a paragrapgh\n",
      "Summarize the importance of Voting in an election: provide me a paragrapgh\n",
      "Summarize the\n"
     ]
    }
   ],
   "source": [
    "text = 'Summarize the importance of Voting in an election: provide me a paragrapgh'\n",
    "print(llm_fa.invoke(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d17445",
   "metadata": {},
   "source": [
    "### `Prompt Template`\n",
    "- A prompt template consists of a `string template`. It accepts `a set of parameters from the user` that can be used to generate a prompt for a language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "39d5e542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to open a restraunt for Italian food. Suggest a fancy name for this.\n",
      "I want to open a restraunt for Chinese food. Suggest a fancy name for this.\n",
      "I want to open a restraunt for {'Bengali', 'Mexican'} food. Suggest a fancy name for this.\n"
     ]
    }
   ],
   "source": [
    "# Actual Text: I want to open a restraunt for chinese food. Suggest a fancy name for this.\n",
    "# Method:1\n",
    "from langchain.prompts import PromptTemplate\n",
    "llm = HuggingFaceHub(repo_id = 'google/flan-t5-large', model_kwargs = {'temperature': 0.8}, huggingfacehub_api_token = HUGGINGFACE_KEY)\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables = ['cuisine'],\n",
    "    template = 'I want to open a restraunt for {cuisine} food. Suggest a fancy name for this.'\n",
    ")\n",
    "p = prompt_template.format(cuisine = \"Italian\") # The user from UI will change only cuisine part.\n",
    "q = prompt_template.format(cuisine = \"Chinese\")\n",
    "r = prompt_template.format(cuisine = {\"Mexican\", \"Bengali\"})\n",
    "print(p)\n",
    "print(q)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c54c00fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is a good name for a company that makes colourful socks'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Method: 2\n",
    "prompt = PromptTemplate.from_template('What is a good name for a company that makes {product}')\n",
    "prompt.format(product = \"colourful socks\") # The user from UI will change only cuisine part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fa3448",
   "metadata": {},
   "source": [
    "### `Chains`\n",
    "- Chains refer to sequences of calls - whether to an LLM, a tool, or a data preprocessing step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "89a51e6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is a good name for a company that makes colourful socks'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = PromptTemplate.from_template('What is a good name for a company that makes {product}')\n",
    "prompt.format(product = \"colourful socks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "72a18a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFaceHub(repo_id = 'google/flan-t5-large', model_kwargs = {'temperature': 0.8}, huggingfacehub_api_token = HUGGINGFACE_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2d18a492",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Own\\.conda\\envs\\genai\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n",
      "C:\\Users\\Own\\.conda\\envs\\genai\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sock mania\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "chain = LLMChain(llm = llm, prompt = prompt)\n",
    "response = chain.run('colourful socks') # Providing input which will be the product\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a0dcc905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWhat is a good name for a company that makes colourful socks\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "What is a good name for a company that makes colourful socks?\n",
      "I've been thinking about making a business that sells colourful socks for men. I'm having a bit of trouble thinking of a good name for this business. I'm not sure if I should use the word \"socks\" in the name or not.\n",
      "7 Answers\n",
      "- 1 month ago\n",
      "I'd call it the \"Sock Man\"\n",
      "- Anonymous1 month ago\n",
      "It depends on the market you're targeting. I'd probably\n"
     ]
    }
   ],
   "source": [
    "chain = LLMChain(llm = llm_fa, prompt = prompt, verbose = True) # Keeping 'verbose = True' provides the flow of the chain mechanism.\n",
    "response = chain.run('colourful socks') # Providing input which will be the product\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3075d2",
   "metadata": {},
   "source": [
    "### `Simple Sequential Chain`\n",
    "- The simplest form of a sequential chain is where each step has a single input and output. `The output of one step is passed as input to the next step` in the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5df99586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Execute more than 1 Prompt Template. Use 'Simple Sequential Chains' Concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d2a00ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFaceHub(repo_id = 'google/flan-t5-large', model_kwargs = {'temperature': 1}, huggingfacehub_api_token = HUGGINGFACE_KEY)\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables = ['cuisine'],\n",
    "    template = 'I want to open a restraunt for {cuisine} food. Suggest a fancy name for this.')\n",
    "name_chain = LLMChain(llm = llm_fa, prompt = prompt_template_name)\n",
    "\n",
    "prompt_template_items = PromptTemplate(\n",
    "    input_variables = ['restaurant_name'],\n",
    "    template = \"Suggest some menu items for {restaurant_name}\") # Using above output as input for this prompt.\n",
    "food_items_chain = LLMChain(llm = llm, prompt = prompt_template_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d9ca23a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spice of India\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "chain = SimpleSequentialChain(chains = [name_chain, food_items_chain])\n",
    "content = chain.run('Indian')\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7c18b9",
   "metadata": {},
   "source": [
    "### `Sequential Chain`\n",
    "-  If we want to check the output of every individual chain, then `'Sequential Chain'` is used.Basically we can print the chain responses of all the chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b2ce0377",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFaceHub(repo_id = 'google/flan-t5-large', model_kwargs = {'temperature': 1}, huggingfacehub_api_token = HUGGINGFACE_KEY)\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables = ['cuisine'],\n",
    "    template = 'I want to open a restraunt for {cuisine} food. Suggest a fancy name for this.')\n",
    "name_chain = LLMChain(llm = llm, prompt = prompt_template_name, output_key = 'restaurant_name')\n",
    "\n",
    "prompt_template_items = PromptTemplate(\n",
    "    input_variables = ['restaurant_name'],\n",
    "    template = \"Suggest some menu items for {restaurant_name}\") # Using above output as input for this prompt.\n",
    "food_items_chain = LLMChain(llm = llm, prompt = prompt_template_items, output_key = 'menu_items')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8d2c5c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Own\\.conda\\envs\\genai\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cuisine': 'Indian', 'restaurant_name': 'Indian House', 'menu_items': 'Chicken Tikka Masala'}\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "from langchain.chains import SequentialChain\n",
    "chain = SequentialChain(\n",
    "    chains = [name_chain, food_items_chain],\n",
    "    input_variables = ['cuisine'],\n",
    "    output_variables = ['restaurant_name', 'menu_items']\n",
    ")\n",
    "print(chain({\"cuisine\": \"Indian\"})) # Providing cuisine name since name_chain which runs first, will take 'cuisine' as input\n",
    "#print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8837a49e",
   "metadata": {},
   "source": [
    "### `Agents & Tools`\n",
    "-  Generally models will be get trained till the certain timeline of data, If we prompt them to ask the recent times information, the model will not be having the answer for that it's not trained on the data based of current events. Hence we need tp ose some APIs from where the LLM can fetch the result and provide it.\n",
    "- Whenever we try to access the real time data, we use `Agents`. If we want to retrieve or use fixed data, we use `RAG Approach`.\n",
    "- In this section we can understand how to implement the 'Agents & Tools' functionality in the application.\n",
    "- https://serpapi.com/ To Access the free search API, use open source serpapi. (>>Api Key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f61310ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentType, initialize_agent, load_tools\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b5463e5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Own\\.conda\\envs\\genai\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 0.3.0. Use Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc. instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThe USA had a GDP value of $1,047,527 in 2023.\u001b[0m\n",
      "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: The USA had a GDP value of $1,047,527 in\u001b[0m\n",
      "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: The USA had a GDP value of $1,047,527 in\u001b[0m\n",
      "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought:The USA had a GDP value of $1,047,527 in\u001b[0m\n",
      "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought:The USA had a GDP value of $1,047,527 in\u001b[0m\n",
      "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought:The USA had a GDP value of $1,047,527 in\u001b[0m\n",
      "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought:The USA had a GDP value of $1,047,527 in\u001b[0m\n",
      "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought:The USA had a GDP value of $1,047,527 in\u001b[0m\n",
      "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought:Thought:The USA had a GDP value of $1,047\u001b[0m\n",
      "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought:Thought:The USA had a GDP value of $1,047\u001b[0m\n",
      "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought:The USA had a GDP value of $1,047,527 in\u001b[0m\n",
      "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought:The USA had a GDP value of $1,047,527 in\u001b[0m\n",
      "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought:The USA had a GDP value of $1,047,527 in\u001b[0m\n",
      "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought:The USA had a GDP value of $1,047,527 in\u001b[0m\n",
      "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought:Thought:The USA had a GDP value of $1,047\u001b[0m\n",
      "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
      "Thought:\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Agent stopped due to iteration limit or time limit.\n"
     ]
    }
   ],
   "source": [
    "# Fetching the results from 'serpapi'.\n",
    "llm = HuggingFaceHub(repo_id = 'google/flan-t5-large', model_kwargs = {'temperature': 1}, huggingfacehub_api_token = HUGGINGFACE_KEY)\n",
    "\n",
    "# Google Search API\n",
    "# 'llm-math' tool uses an LLM,\n",
    "tools = load_tools(['serpapi', 'llm-math'], llm = llm, serpapi_api_key = SERP_API_KEY)\n",
    "\n",
    "# Initialize an Agent with the tools,the language model, and the type of agent we want to use.\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True, handle_parsing_errors=True)\n",
    "# agent = initialize_agent(tools, llm, agent = AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose = True)\n",
    "response = agent.run('What was the GDP Value of the USA in the year 2023?')\n",
    "# response = agent.run('Who was the president of India in the year 2011?')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "90b22a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThe Departed was released in 2005.\u001b[0m\n",
      "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe Departed was released in 2005.\u001b[0m\n",
      "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
      "Thought:\u001b[32;1m\u001b[1;3mIn which year was the film The Departed with Lenonardo Dicapri\u001b[0m\n",
      "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe Departed was released in 2005.\u001b[0m\n",
      "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought:The Departed was released in 2005. Observation: Invalid Format\u001b[0m\n",
      "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought:The Departed was released in 2005. Observation: Invalid Format\u001b[0m\n",
      "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought:The Departed was released in 2005. Observation: Invalid Format\u001b[0m\n",
      "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought:The Departed was released in 2005. Observation: Invalid Format\u001b[0m\n",
      "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought:The Departed was released in 2005. Observation: Invalid Format\u001b[0m\n",
      "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought:The Departed was released in 2005. Observation: Invalid Format\u001b[0m\n",
      "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought:The Departed was released in 2005. Observation: Invalid Format\u001b[0m\n",
      "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought:The Departed was released in 2005. Observation: Invalid Format\u001b[0m\n",
      "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought:The Departed was released in 2005. Observation: Invalid Format\u001b[0m\n",
      "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought:The Departed was released in 2005. Observation: Invalid Format\u001b[0m\n",
      "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought:The Departed was released in 2005. Observation: Invalid Format\u001b[0m\n",
      "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
      "Thought:\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Agent stopped due to iteration limit or time limit.\n"
     ]
    }
   ],
   "source": [
    "# Fetching the results from 'wikipedia'.\n",
    "llm = HuggingFaceHub(repo_id = 'google/flan-t5-large', model_kwargs = {'temperature': 7}, huggingfacehub_api_token = HUGGINGFACE_KEY)\n",
    "\n",
    "# Google Search API\n",
    "# 'llm-math' tool uses an LLM,\n",
    "tools = load_tools(['wikipedia', 'llm-math'], llm = llm, serpapi_api_key = SERP_API_KEY)\n",
    "\n",
    "# Initialize an Agent with the tools,the language model, and the type of agent we want to use.\n",
    "# agent = initialize_agent(tools, llm, agent = AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose = True)\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True, handle_parsing_errors=True)\n",
    "response = agent.run('In which year was the film The Departed with Lenonardo Dicaprio releared?')\n",
    "# response = agent.run('Who was the president of India in the year 2011?')\n",
    "# response = agent.run('What was the GDP Value of the USA in the year 2023?')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66048cff",
   "metadata": {},
   "source": [
    "## `Memory`\n",
    "-  Imagine we ask a code to the model and it provides the results in a particular format. Again if we need to get same code but in a different format (which we specify in a prompt), then the it provides the result. This is possible since the first code provided by the model will be stored in it's memory.\n",
    "- In this section we can understand how to implement the memory functionality in the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "147f6fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables = ['cuisine'],\n",
    "    template = 'I want to open a restraunt for {cuisine} food. Suggest a fancy name for this'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "360511c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mexican restaurant\n"
     ]
    }
   ],
   "source": [
    "# 1st Execution\n",
    "from langchain.chains import LLMChain\n",
    "chain = LLMChain(llm = llm, prompt = prompt_template_name)\n",
    "name = chain.run('Mexican')\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "479d360a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indian House\n"
     ]
    }
   ],
   "source": [
    "# 2nd Execution\n",
    "name = chain.run('Indian')\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f8690b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No information is saved in the Memory.\n",
    "type(chain.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0db9f5",
   "metadata": {},
   "source": [
    "### `Conversation Buffer Memory`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ea00e056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mexican restaurant\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "chain = LLMChain(llm = llm, prompt = prompt_template_name, memory = memory)\n",
    "name = chain.run('Mexican')\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "428a7a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bengali House\n"
     ]
    }
   ],
   "source": [
    "# 2nd Execution\n",
    "name = chain.run('Bengali')\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "34c0c63f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Mexican\n",
      "AI: Mexican restaurant\n",
      "Human: Bengali\n",
      "AI: Bengali House\n"
     ]
    }
   ],
   "source": [
    "# This provides the complete conversation stored as Memory as chat history.\n",
    "print(chain.memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407569de",
   "metadata": {},
   "source": [
    "### `Conversation Chains`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d512d0cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "{history}\n",
      "Human: {input}\n",
      "AI:\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "\n",
    "convo = ConversationChain(llm = llm)\n",
    "print(convo.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "11d6d38d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The first cricket world cup was won by India.'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run('Who won the first cricket world cup?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "878898c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5+5 is a number that is repeated indefinitely.'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run('How much is 5+5?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8ae76f9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The captain of the winning team was Virat Kohli.'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we ask next question, it will related to the prompt which we asked earlier and provide the results.\n",
    "convo.run('Who was the captain of winning team?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580a7531",
   "metadata": {},
   "source": [
    "### `Conversation Buffer Window Memory`\n",
    "- Conversation Buffer Memory stores the all chat information in it's memory which leads to consumtion of memory. To overcome this, we can specify the certain number of chats or conversations which is to be stored in its memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e6c43a85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The first cricket world cup was won by India.'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k = 2) # To save 2 prompts in the memory.\n",
    "\n",
    "convo = ConversationChain(\n",
    "    llm = llm,\n",
    "    memory = memory)\n",
    "convo.run('Who won the first cricket world cup?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3715df51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The value of 5+5 is 125.'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run('what is value if we add 5+5?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8fc6c78a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The first cricket world cup was played in a total of 66 matches.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run('How many matches were played?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6519b36b",
   "metadata": {},
   "source": [
    "### `Document Loaders`\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c4c135d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='M.Tech.\\nData Science & Engineering\\nfor W\\norking\\nProfessionals', metadata={'source': 'Bits Mtech.pdf', 'page': 0}),\n",
       " Document(page_content='INDEX\\nProgramme introduction\\nWho should apply\\nProgramme highlights \\nProgramme objectives\\nMode of learning\\nExperiential learning\\nProgramme curriculum\\nEligibility criteria\\nFee structure\\nCourse-wise Syllabus \\nMode of examination\\nHow to apply\\nStudents speak01\\n02\\n03\\n05\\n06\\n08\\n09\\n10\\n11\\n12\\n20\\n21\\n24', metadata={'source': 'Bits Mtech.pdf', 'page': 1}),\n",
       " Document(page_content=\"Program Introduction\\n01Data Science ranks as the fastest-growing global job, with a remarkable \\ngrowth rate exceeding 650% since 2012, as per LinkedIn's Emerging Jobs \\nreport. \\nThe Global Data Science Platform market size is projected to rise at a\\nCAGR of 27.7% to an anticipated USD 322.80 billion by 2026, indicating\\na significant upward trend.\\nJoin BITS Pilani WILP's M.Tech. Data Science & Engineering Programme,\\na comprehensive four-semester course designed for Software and IT \\nprofessionals. \\nThis program empowers professionals to specialize in roles like Data Analyst, \\nData Engineer, Data Architect, and Data Scientist, offering an opportunity to \\nenhance career prospects without the need for a career break.\\nM.Tech. Data Science and Engineering APPLY NOW\", metadata={'source': 'Bits Mtech.pdf', 'page': 2}),\n",
       " Document(page_content='02Software and IT \\nprofessionals in roles \\nlike Software Engineer, \\nProgrammer, Software \\nTest Engineer, Support \\nEngineer, Data Analyst, \\nor Business Analyst \\nseeking to transition to \\nData Scientist or Data \\nEngineer roles should \\nconsider applying to this \\nprogram.Who Should Apply?\\nM.Tech. Data Science and Engineering APPLY NOW', metadata={'source': 'Bits Mtech.pdf', 'page': 3}),\n",
       " Document(page_content='Programme  Highlights \\n03Covers fundamental to advanced \\nskill sets and comprehensive \\nknowledge in Data Science.Aimed at transitioning software & \\nIT professionals into Data Science \\ncareers tracks closest to their \\ninterest/passion. Pursue the four-semester\\nprogramme without any career\\nbreak. Contact classes over a\\ntechnology enabled platform are\\nconducted mostly on weekends\\nand after business hours.The program includes core and \\nelective courses such as Data \\nManagement for Machine \\nLearning, Big Data Systems, \\nOptimization Techniques for \\nAnalytics, Natural Language \\nProcessing, etc.The curriculum covers key \\nareas for popular Data Science \\nroles like Analyst, Engineer, \\nArchitect, and Scientist.\\nOffers the most comprehensive\\nData Science Curriculum for\\nworking professionals.The program utilizes various \\nTools and Technologies like \\nApache Spark, Apache Storm for \\nBig Data Systems/Real-time \\nProcessing, Tableau for data \\nvisualization, Tensorflow for Deep \\nLearning, and various Python \\npackages for data processing, \\nmachine learning, and \\nvisualization.M.Tech. Data Science and\\nEngineering is a BITS Pilani Work \\nIntegrated Learning Programme \\n(WILP). BITS Pilani Work \\nIntegrated Learning Programmes \\nare UGC approved.\\nM.Tech. Data Science and Engineering APPLY NOW', metadata={'source': 'Bits Mtech.pdf', 'page': 4}),\n",
       " Document(page_content='04The program combines classroom\\nand experiential learning through\\nlabs, assignments, case studies,\\nand work-integrated activities.Option to submit fee using easy-\\nEMI with 0% interest and 0 down \\npaymentThe Continuous Evaluation \\nSystem assesses learners at \\nregular intervals, providing \\ntimely feedback to busy \\nprofessionals.Participants who successfully \\ncomplete the programme will \\nbecome members of an elite & \\nglobal community of BITS Pilani \\nAlumni.The final-semester Dissertation \\n(Project Work) allows students to \\napply learned concepts and \\ntechniques.\\nM.Tech. Data Science and Engineering APPLY NOW', metadata={'source': 'Bits Mtech.pdf', 'page': 5}),\n",
       " Document(page_content='05Programme  Objectives \\nData structures, algorithms, and \\ncomplexity management.Mathematical and Statistical \\nmodeling for problem-solving.\\nData Mining involving \\npreprocessing, classification, and \\nengineering through distributed and \\ncloud computing.Computer organization, \\narchitecture, and advanced data \\nprocessing techniques.The program focuses on essential skills required for lucrative roles in Data Science, Data \\nEngineering, and Advanced Analytics, covering:\\nAdvanced skills in Machine \\nLearning, Artificial Intelligence,\\nDeep Learning, and Natural \\nLanguage Processing.\\nM.Tech. Data Science and Engineering APPLY NOW', metadata={'source': 'Bits Mtech.pdf', 'page': 6}),\n",
       " Document(page_content='06M.Tech. Data Science and Engineering APPLY NOWMode of Learning\\nThe Mode of Learning used in this programme is called - Work Integrated Learning. \\nInternationally, Work Integrated Learning (WIL) is defined as \"An educational approach \\ninvolving three parties - the student, educational institution, and employer organization(s) \\n- consisting of authentic work-focused experiences as an intentional component of the \\ncurriculum. Students learn through active engagement in purposeful work tasks, which \\nenable the integration of theory with meaningful practice that is relevant to the students\\' \\ndiscipline of study and/or professional development*.\\nAn education model can be considered as WIL if and only if:\\n1. The programs are designed and developed by the institute in collaboration with \\nindustry.\\n2. Work-focused experiences form an active part of the curriculum.\\n3. The program structure, pedagogy and assessment enable integration of theory-with \\nrelevant practice.\\nThe innovative Work Integrated Learning Programs (WILP) of BITS Pilani are quite \\naligned with the above definition and requirements. The programs are designed in \\ncollaboration with its industry partners, subject matter experts from industry and \\nacademia that enable the students to remain relevant in their chosen profession, grow in \\ntheir career and retain the habit of lifelong learning. The continued availability of \\nworkplace related experiences along with the weekly instruction sessions promote \\nintegration of theory with practice. An active participation of the organization mentor in the \\nlearning process of the student plays a key role. Case studies, simulation exercises, labs \\nand projects further strengthen this integration.\\nThe WILP of BITS Pilani is comparable to its campus-based programs in terms of \\nstructure, rigor, instruction, labs, assessment, faculty profile and learning support. The \\npervasive adoption of technology in all its academic processes makes the same \\nhigh-quality education of BITS Pilani available to the aspirants at scale with the required \\nflexibility.', metadata={'source': 'Bits Mtech.pdf', 'page': 7}),\n",
       " Document(page_content='07M.Tech. Data Science and Engineering APPLY NOW● Can pursue the programme without any career break and along with the job.\\n● The programme curriculum is highly relevant to sectors, industries and  \\norganisations they work for\\n● In addition to the institute, the learning experience of working professionals in the \\nprogramme is also supported by the employer organisation and Industry Mentors.\\n● Effective use of technology to deliver a range of learning interventions at the location \\nof the working professional such as faculty contact sessions, asynchronous learning \\nmaterials, remote, virtual and cloud labs, Learner support, peer to peer collaboration \\netc. \\n● Contact sessions with faculty take place mostly over weekends or after business \\nhours and are conducted over a technology platform that can be accessed from \\nanywhere. \\n● Mid semester and End semester examinations for every semester are conducted \\nmostly at designated examination centres distributed across the country (for details \\nrefer to link mode of examinations) \\n● Learners can access engaging learning material which includes recorded lectures \\nfrom BITS Pilani faculty members, course handouts and recorded lab content where \\napplicable.\\nKey Benefits of BITS Pilani WILP', metadata={'source': 'Bits Mtech.pdf', 'page': 8}),\n",
       " Document(page_content=\"Project Work\\nThe fourth semester offers an opportunity for learners to apply \\ntheir knowledge gained during the programme to a real-world \\nlike complex project. The learner is expected to demonstrate \\nunderstanding of vital principles learnt across semesters and\\ntheir ability to successfully apply these conceptsCase studies & Assignments\\nCarefully chosen real-world cases & assignments are both discussed and used as problem-\\nsolving exercises during the programme.\\nContinuous Assessment \\nThe learners' performance is assessed continuously throughout the semester using various tools \\nsuch as quiz, assignments, mid-semester and comprehensive exams. The assessment results are \\nshared with the learners to improve their performance. Each course will entail a minimum of 1 \\nAssignment/ Quiz, a Mid-semester exam and a final Comprehensive exam. Your semester \\ncalendar will indicate the dates of the Mid-semester and Comprehensive exam. Online Exams \\nfacility available.\\nTypically, a Mid-semester or Comprehensive examination for a course is for 2-3 hours duration. \\nThe examinations are typically conducted over a weekend, i.e. Saturday and Sunday.Experiential  Learning\\nThe programme emphasizes on Experiential Learning that allows learners to apply concepts \\nlearnt in classroom in simulated and real work situations. This is achieved through:\\n08Tools & Technologies covered\\nM.Tech. Data Science and Engineering APPLY NOW\", metadata={'source': 'Bits Mtech.pdf', 'page': 9}),\n",
       " Document(page_content=\"09Programme  Curriculum\\nThird Semester\\n● Big Data Systems \\n● Elective - IV \\n● Elective - V \\n● Elective - VIFourth Semester\\n● Dissertation\\nGeneral Pool of Electives\\n● Data Warehousing\\n● Graphs – Algorithms and Mining\\n● Deep Learning*\\n● Probabilistic Graphical Models\\n● Optimization Techniques for Analytics\\n● Data Management for Machine Learning● Natural  Language Processing\\n● Data Visualization and Interpretation \\n● Stream Processing and Analytics\\n● Information Retrieval\\n● Artificial and Computational Intelligence\\n● Machine Learning*\\n● Applied Machine Learning The programme features 12 courses between Semester 1-3, and a Dissertation in Semester 4.\\nAll the courses will be offered using contact sessions over a technology enabled platform.\\nNote: *Machine Learning course is a prerequisite for Deep Learning elective course.\\nNote: Choice of Electives is made available to enrolled students at the beginning of each semester. Students' choice \\nwill be taken a s one of the factors while deciding on the Electives offered. However, Electives finally offered will be \\nat the discretion of the Institute.Second Semester\\n● Introduction to Statistical Methods \\n● Elective - I\\n● Elective - II \\n● Elective - IIIFirst Semester\\n● Mathematical foundations for\\n Data Science \\n● Introduction to Data Science\\n● Computer Organization and System\\n Software \\n● Data Structures and Algorithm  \\n Design\\nM.Tech. Data Science and Engineering APPLY NOW\", metadata={'source': 'Bits Mtech.pdf', 'page': 10}),\n",
       " Document(page_content='Eligibility Criteria\\n● Employed professionals holding B.E. / B.Tech. with at least 60% aggregate marks and \\nminimum one-year relevant work experience after the completion of the degree are \\neligible to apply. \\n● Employed professionals holding MCA / M.Sc. or equivalent with at least 60% \\naggregate marks with university level mathematics / statistics as mandatory subjects \\nand minimum one-year relevant work experience after the completion of the degree \\nare also eligible to apply.\\n10M.Tech. Data Science and Engineering APPLY NOW', metadata={'source': 'Bits Mtech.pdf', 'page': 11}),\n",
       " Document(page_content='11Application Deadline: 18th March 2024Click here      to learn moreFee Structure\\nFee Structure for students admitted in Academic Year 2023-2024 is as follows:\\nEasy Monthly Payment Option with 0% Interest and 0 Down Payment\\nInstant EMI option with 0% interest and 0 Down Payment is now available\\nthat allows you to pay programme fee in an easy and convenient way.\\n● Instant online approval in seconds \\n● No Credit Cards/ CIBIL score required \\n● Easy and secure online process using Aadhaar and PAN number \\n● Anyone with a Salary Account with Netbanking can apply the Option to submit fee\\n using easy- EMI with 0% interest and 0 down payment\\nAll the above fees are non-refundable.\\nImportant: For every course in the programme, institute will recommend textbooks,\\nstudents would need to procure these textbooks on their own.Application Fees\\n(one time)Admission Fees\\n(one time)Semester Fees\\n(per semester)\\n`1500 `16,500 `68,500\\nM.Tech. Data Science and Engineering APPLY NOW', metadata={'source': 'Bits Mtech.pdf', 'page': 12}),\n",
       " Document(page_content='1212Course-wise Syllabus \\nMathematical Foundations for Data Science\\n● Vector and matrix algebra, systems of linear algebraic equations and their solutions\\n● Eigenvalues, eigenvectors, and diagonalization of matrices\\n● Multivariate calculus, vector calculus, Jacobian and Hessian, multivariate Taylor  \\n series, gradient descent, unconstrained optimization, constrained optimization,  \\nnonlinear optimization, stochastic gradient descent, dimensionality reduction and \\nPCA, optimization for support vector machines\\nIntroduction to Statistical Methods\\n● Basic probability concepts \\n● Conditional probability\\n● Bayes Theorem\\n● Probability distributions\\n● Continuous and discrete distributions\\n● Transformation of random variables\\n● Estimating mean\\n● Variance \\n● Covariance\\n● Hypothesis Testing Maximum likelihood \\n● ANOVA single factor, dual factor \\n● Time series analysis: AR, MA, ARIMA, SARIMA\\n● Sampling based on distribution \\n● Statistical significance\\n● Gaussian Mixture Model\\n● Expectation Maximization\\nData Warehousing\\n● Introduction, evolution of data warehousing\\n● decision support systems \\nM.Tech. Data Science and Engineering APPLY NOW', metadata={'source': 'Bits Mtech.pdf', 'page': 13}),\n",
       " Document(page_content='12● Goals, benefit, and challenges of data warehousing \\n● Architecture\\n● Aata warehouse information flows \\n● Software and hardware requirements\\n● Approaches to data warehouse design\\n● Creating and maintaining a data warehouse\\n● Online Analytical Processing (OLAP) and multi-dimensional data, multi-dimensional \\nmodeling\\n● View materialization\\n● Data marts \\n● Data warehouse metadata\\n● Data mining\\nComputer Organization & Software Systems\\n● Programmer model of CPU\\n● Basic concept of buses and interrupts\\n● Memory subsystem organization\\n● I/O organization\\n● Concept of assembler, linker & loader\\n● Types of operating systems\\n● Concept of process\\n● OS functions: Process scheduling, Memory management, I/O management and \\nrelated issues\\nGraphs - Algorithms and Mining\\n● Basic concepts of graphs and digraphs connectivity, reachability and vulnerability\\n● Trees, tournaments and matroids\\n● Planarity\\n● Routing and matching problems\\n● Representations\\n● Various algorithms Applications\\n● Introduction to graph mining, Graph Pattern Mining, Graph Classification, Graph \\nCompression, graph model, graph dynamics, social network analysis, visualization, \\nsummarization, graph clustering, link analysis, applications of graph patterns\\n13M.Tech. Data Science and Engineering APPLY NOW', metadata={'source': 'Bits Mtech.pdf', 'page': 14}),\n",
       " Document(page_content='12Big Data Systems \\n● What is big data - are existing systems sufficient?\\n● Data Warehouse v/s Data Lakes\\n● Hadoop – Components\\n● Storage - Relational DBs/ NoSQL dbs / HDFS / HBase / Object Data stores – S3 \\n● Serialization\\n● Interfaces - Hive/ Pig\\n● Stream Processing\\n● Spark\\n● Mahout\\nDeep Learning \\n● Common Architectural Principles of Deep Networks\\n● Building Blocks of Deep Networks\\n● Convolutional Neural Networks (CNNs)\\n● Recurrent Neural Networks\\n● Recursive Neural Networks\\n● Building Deep Networks with ND4J\\n● Applications to Sequence Data\\n● Anomaly Detection\\n● Tuning Deep Networks\\n● Vectorization\\nProbabilistic Graphical Models \\n● HMM, Markov Random Field, Bayesian networks, Representation, Learning, \\nInference\\n● Dynamic Bayesian Networks and Temporal Bayesian networks, applications\\nOptimization Techniques for Analytics\\n● Role of optimization in different types of analytics \\n● Introduction to Linear Programming \\n● LP Model and graphical solution \\n● Primal Simplex method\\n14M.Tech. Data Science and Engineering APPLY NOW', metadata={'source': 'Bits Mtech.pdf', 'page': 15}),\n",
       " Document(page_content='1215● Dual Simplex and Post Optimality Analysis, Revised Simplex method with examples\\n● Application of linear programming in transportation \\n● Assignment problems\\n● Integer linear programming \\n● Mixed integer programming\\n● complexity analysis, branch and bound techniques\\n● Goal programming\\n● Network models - critical path method and PERT \\n● Dynamic programming \\n● Game theory\\n● Additional meta heuristic techniques\\n● 2-3 case studies from relevant industry domains\\nData Management for Machine Learning\\n● Data Models and Query Languages: Relational, Object-Relational, NoSQL data \\nmodels\\n● Declarative (SQL) and Imperative (Map Reduce) Querying\\n● Data Encoding: Evolution, Formats, Models of dataflow\\n● Machine learning workflow\\n● Data management challenges in ML workflow\\n● Data Pipelines and patterns\\n● Data Pipeline Stages: Data extraction, ingestion, cleaning, wrangling, versioning, \\ntransformation, exploration, feature management\\n● Modern Data Infrastructure: Diverse data sources, Cloud data warehouses and lakes, \\nData Ingestion tools, Data transformation and modelling tools, Workflow orchestration \\nplatforms\\n●  ML model metadata and Registry, ML Observability, Data privacy and anonymity\\nM.Tech. Data Science and Engineering APPLY NOW M.Tech. Data Science and Engineering APPLY NOW', metadata={'source': 'Bits Mtech.pdf', 'page': 16}),\n",
       " Document(page_content='1216Natural Language Processing \\n● Natural Language Understanding and Generation\\n● N-gram and Neural Language Models\\n● Word to Vectors / Word Embedding (Skip gram/CBOW, Glove, BERT/ XLM, MURIL), \\n● Part of Speech Tagging\\n● Hidden Markov Models \\n● Parsing - Syntactic, Statistical, Dependency, Word Sense Disambiguation, Semantic \\nWeb Ontology\\nIntroduction to Data Science\\n● Data Analytics\\n● Data and Data Models \\n● Data wrangling\\n● Feature Engineering \\n● Classification and Prediction\\n● Association Analysis\\n● Clustering\\n● Anomaly Detection\\n● Exploratory / explanatory data analysis with visual storytelling\\n● Ethics for Data Science\\nInformation Retrieval \\n● Organization, representation, and access to information\\n● Categorization, indexing, and content analysis\\n● Data structures for unstructured data\\n● Design and maintenance of such data structures, indexing and indexes, retrieval, and \\nclassification schemes\\n● Use of codes, formats, and standards\\n● Analysis, construction and evaluation of search and navigation techniques\\n● Search engines and how they relate to the above. Multimedia data and their \\nrepresentation and search\\nM.Tech. Data Science and Engineering APPLY NOW', metadata={'source': 'Bits Mtech.pdf', 'page': 17}),\n",
       " Document(page_content='1217Data visualization and Interpretation \\n● Visualization as a Discovery tool\\n● Visualization skills for the masses\\n● The Visualization methodology\\n● Visualization design objectives\\n● Exploratory vs. explanatory analysis\\n● Understanding the context for data presentations, 3-minute story, Effective Visuals\\n● Gestalt principles of visual perception, Visual Ordering, Decluttering, Story Telling, \\nVisualization Design\\nStream Processing and Analytics\\n● Real Time, Streaming Data & Sources, Real time streaming system architecture, \\nCharacteristics of a Real Time Architecture and Processing \\n● Configuration and Coordination Systems: Distributed State and Issues, Coordination \\nand Configuration using Apache ZooKeeper\\n● Data Flow Management: Distributed Data Flows, Various Data Delivery and \\nProcessing Requirements, N+1 Problem, Apache Kafka (High-Throughput Distributed \\nMessaging)\\n● Processing Stream Data with Storm\\n● Overview of Data Storage – Requirements: Need for long-term storage for a real time \\nprocessing framework, In-memory Storage, No-Sql Storage Systems, Choosing a \\nright storage solution\\n● Visualizing Data: Requirements, Principles and tools\\n● Bounds of Random variables, Poisson Processors, Maintaining Simple Statistics from \\nData Streams, Sliding Windows and computing statistics over sliding windows, Data \\nSynopsis (Sampling, Histograms, Wavelets, DFT), Exact Aggregation, Timed \\nCounting and Summation, Multi Resolution Time Series Aggregation, Stochastic \\nOptimization \\nM.Tech. Data Science and Engineering APPLY NOW', metadata={'source': 'Bits Mtech.pdf', 'page': 18}),\n",
       " Document(page_content='1218● Statistical Approximation to Streaming Data: Probabilities and Distributions, Sampling \\nProcedures for Streaming Data, Approximating Streaming Data with Sketching, \\nRegisters and Hash Functions, Working with Sets, The Bloom Filter, Distinct Value \\nSketches, The Count-Min Sketch\\n● Clustering techniques for Streaming Data; Classification methods: Decision Tree \\n(VFDT) Evaluating stream processing algorithms\\n● Case Studies in Designing solutions to streaming data\\nArtificial and Computational Intelligence \\n● Agents and environments, Task Environments, Working of agents\\n● Uninformed Search Algorithms: Informed Search\\n● Local Search Algorithms & Optimization Problems: Genetic Algorithm\\n● Searching with Non-Deterministic Actions, Partial Information and Online search \\nagents, Game Playing, Constraint Satisfaction Problem, Knowledge Representation \\nusing Logics: TT-Entail for inference from truth table, Proof by resolution, Forward \\nChaining and Backward Chaining, Inference in FOL, Unification & Lifting, Forward \\nchaining, Backward Chaining, Resolution\\n● Probabilistic Representation and Reasoning: Inference using full joint distribution, \\nRepresentation of Conditional Independence using BN, Reinforcement Learning\\n● Difference between crisp and fuzzy logic, shapes of membership function, \\nFuzzification and defuzzification, fuzzy logic reasoning\\n● Decision making with fuzzy information, Fuzzy Classification\\n● Connectionist Models: Introduction to Neural Networks, Hopfield Networks, \\nPerceptron Learning, Backpropagation & Competitive Learning\\n● Applications of Neural Net: Speech, Vision, Traveling Salesman\\n● Genetic Algorithms - Chromosomes, fitness functions, and selection mechanisms\\n● Genetic algorithms: crossover and mutation, Genetic programming\\nMachine Learning \\n● Introduction to Machine Learning, Various kinds of learning, Supervised Learning, \\nUnsupervised Learning, Model Selection\\n● Bayesian Learning\\n● MAP Hypothesis, MDL Principle, Bias Variance Decomposition, Bayes Optimal \\nClassifier, Naive Bayes Classifier\\n● Linear Models for Regression, Linear Models for Classification\\n● Non-Linear models, Decision trees\\nM.Tech. Data Science and Engineering APPLY NOW', metadata={'source': 'Bits Mtech.pdf', 'page': 19}),\n",
       " Document(page_content='1219● Instance Based Learning, KNN Algorithm, CBR Learning\\n● Support Vector Machines, VC Dimension; Neural Networks, Perceptron Learning, \\nBack Propagation Algorithm\\n● Introduction to Genetic Algorithms\\nApplied Machine Learning \\n● Need for machine learning\\n● Prediction and classification methods\\n● Use cases in application domains\\n● Interpretation of results\\n● Limitations of various techniques\\n● End to end Machine learning - data collection, data preparation, model selection\\nDissertation\\nA student registered in this course must take a topic in an area of professional interest \\ndrawn from the on the job work requirement which is simultaneously of direct relevance to \\nthe degree pursued by the student as well as to the employing / collaborating \\norganization of the student and submit a comprehensive report at the end of the semester \\nworking under the overall supervision and guidance of a professional expert who will be \\ndeemed as the supervisor for evaluation of all components of the dissertation. Normally \\nthe Mentor of the student would be the Dissertation supervisor and in case Mentor is not \\napproved as the supervisor, Mentor may play the role of additional supervisor. The final \\ngrades for dissertation are non-letter grades namely Excellent, Good, Fair and Poor, \\nwhich do not go into CGPA computation.\\nM.Tech. Data Science and Engineering APPLY NOW', metadata={'source': 'Bits Mtech.pdf', 'page': 20}),\n",
       " Document(page_content='12During these semesters,\\nin addition to the above\\nmentioned Mid-Semester and \\nComprehensive examinations, \\nthere will also be \\nQuizzes/Assignments conducted\\nonline on the Learning \\nManagement System (LMS) as per \\nthe course plan in which the \\nstudents need to participate.Mode of Examinations \\napplicable for students \\nadmitted in\\nBatch starting in\\nApril/May 2024.   Semester 1, 2 and 3 have Mid-Semester \\nExaminations and Comprehensive \\nExaminations for each course.\\nThese examinations are mostly \\nscheduled on Friday, Saturday or \\nSunday. Students need to appear in \\nperson for taking the examinations\\nat the institution’s designated \\nexamination centres as per the \\nexamination schedule, Instructions, rules \\nand guidelines announced before every \\nexamination. \\nMode of Examination\\nIn Semester 4 (Final \\nSemester), the student will be \\ndoing Dissertation/Project \\nWork as per the Institution’s \\nguidelines.\\n20● Students can take their examination at any of our 23 designated examination centres in \\nIndia at the following locations:  \\n● South Zone: Bangalore, Chennai, Hyderabad, Vijayawada, Visakhapatnam, Kochi, \\nThiruvananthapuram and Coimbatore.\\n North Zone: Delhi NCR, Jaipur, Chandigadh, Lucknow and Pilani.\\n West Zone: Mumbai, Pune, Goa, Ahmedabad, Indore and Nagpur.\\n East Zone: Kolkata, Bhubaneshwar, Guwahati and Jamshedpur.\\n In addition to these locations, the Institution also has a designated examination\\n centre in Dubai.\\nM.Tech. Data Science and Engineering APPLY NOW', metadata={'source': 'Bits Mtech.pdf', 'page': 21}),\n",
       " Document(page_content='12How to Apply\\nStep1 Step2 Step3 Step4\\nDownload a PDF \\ncopy of the application form.Pay the application fee of INR 1,500 using Net banking/Debit Card/Credit Card.Print the downloaded Application Form and note your Application Form Number.Fill and submit your application form for your chosen program.\\nIn the printout of the downloaded Application Form, you will notice on page no. 3 a section called the \\nEmployer Consent Form. Complete the Employer Consent Form. This form needs to be signed and stamped by your organisation’s HR or any other authorised signatory of the company.\\nImportant: In view of work-from-home policies mandated by many organisations, a few candidates may \\nnot be able to get the physical forms signed by their HR/other authorised organisational representative. Such candidates may instead request an email approval to be sent to their official email ID by the HR using the format available through this link.\\nOn page 4, complete the Mentor Consent Form, \\nwhich needs to be signed by your Mentor.Due to remote work policies, some candidates may struggle to get physical mentor signatures. They can request email approval using a provided format.Create your login at the \\nApplication Center by \\nentering your unique\\nEmail id and create a \\npassword of your \\nchoice.Once logged in, \\nfollow four essential \\nsteps:\\n21M.Tech. Data Science and Engineering APPLY NOWClick here to  \\napply now', metadata={'source': 'Bits Mtech.pdf', 'page': 22}),\n",
       " Document(page_content='Who is a mentor:\\n● Candidates applying to Work Integrated Learning Programmes must choose a Mentor, \\nwho will monitor the academic progress of the candidate, and act as an advisor & \\ncoach for successful completion of the programme.\\n● Candidates should ideally choose the immediate supervisor or another senior person \\nfrom the same organisation. In case a suitable mentor is not available in the same \\norganisation, a candidate could approach a senior person in another organisation who \\nhas the required qualifications. Wherever the proposed Mentor is not from the same \\nemploying organization as that of the candidate, a supporting document giving \\njustification for the same should be provided by the candidate’s employer.\\n● Candidates applying to B.Tech. programmes should choose a Mentor who is an \\nemployed professional with B.E./ B.S./ B.Tech./ M.Sc./ A.M.I.E./ Integrated First \\nDegree of BITS or equivalent. Candidates applying to M.Tech., M.Sc., MBA, M.Phil \\nprogramme should choose a Mentor who is an employed professional with:\\n● B.E./ M.Sc./ M.B.A./ M.C.A./ M.B.B.S. etc. and with a minimum of five years of relevant \\nwork experience.\\n OR\\n● M.E./ M.S./ M.Tech./ M.Phil./ M.D./ Higher Degree of BITS or equivalent.\\nPage 5 of the downloaded \\nApplication Form includes \\na Checklist of \\nEnclosures/Attachments.Photocopies of these \\ndocuments should be made, \\nand applicants need to \\nself-attest academic mark \\nsheets and certificates.In the final step (Step 4),\\nupload your printed Application \\nForm, Mentor Consent Form, \\nEmployer Consent Form, and \\nmandatory documents \\none by one.\\nAccepted file formats for \\nuploads include .DOC, \\n.DOCX, .PDF, .ZIP, and \\n.JPEG.The Admissions Cell will \\nreview your application for \\ncompleteness, accuracy, \\nand eligibility.Selected candidates will \\nreceive email notifications \\nwithin two weeks of submitting \\ntheir application with all \\nrequired documents.\\nYou can also check your \\nselection status by logging \\nin to the Online Application \\nCentre.\\n22M.Tech. Data Science and Engineering APPLY NOW', metadata={'source': 'Bits Mtech.pdf', 'page': 23}),\n",
       " Document(page_content='UGC Approval\\nBITS Pilani is an Institution of Eminence under UGC (Institution of Eminence \\nDeemed to be Universities) Regulations, 2017. The Work Integrated Learning \\nProgrammes (WILP) of BITS Pilani constitutes a unique set of educational \\nofferings for working professionals. WILP are an extension of programmes \\noffered at the BITSPilani Campuses and are comparable to our regular \\nprogrammes both in terms of unit/credit requirements as well as academic \\nrigour.  In addition, it capitalises and further builds on practical experience of \\nstudents through high degree of integration, which results not only in \\nupgradation of knowledge, but also in up skilling, and productivity increase. \\nThe programme may lead to award of degree, diploma, and certificate in \\nscience, technology/engineering, management, and humanities and social \\nsciences.\\nOn the recommendation of the Empowered Expert Committee, UGC in its \\n548th Meeting held on 09.09.20 has approved the continued offering of BITS \\nPilani’s Work Integrated Learning programmes.\\nThe material in this brochure is provided for educational and informational purposes only. All the \\nimages that have been used belong to their respective owners and have been picked up from the \\npublic domain.\\n23M.Tech. Data Science and Engineering APPLY NOW', metadata={'source': 'Bits Mtech.pdf', 'page': 24}),\n",
       " Document(page_content='Abhishek Kumar Mishra\\nLeader, Data Science, Brilio TechnologiesI truly see my decision of pursuing a degree programme from BITS \\nPilani WILP as the best career move so far. This programme equipped me with world-class skills, technologies and in-depth knowledge in this field that helped me to become the leader. Learning without a career break from the best faculty to weekend classes, recorded lectures, everything was just perfect!C R Bhargavi\\nSoftware Engineer, Ford Motor Pvt LtdStudents Speak\\nI was looking for an upskilling programme that meets my career aspirations without any career break and BITS Pilani WILP was exactly what I needed. My experience with the faculty was great and the concepts I learnt here are relevant in real-time situations as well.\\nPrashant Thakre\\nTechnical Architect, GE HealthcareI was looking for a program where I could learn multiple things relevant to my work profile. I then heard of BITS Pilani WILP. My overall journey with WILP has been amazing. I’ve learned a lot and now I am more confident at my workplace. I am able to explain complex things to people easily, which is ultimately helping my entire team to perform better.\\nB2C_05_02_202424M.Tech. Data Science and Engineering APPLY NOW', metadata={'source': 'Bits Mtech.pdf', 'page': 25}),\n",
       " Document(page_content=\"Let's start a conversation\\nto ignite the change you desire\\nCall: 080-48767777https://bits-pilani-wilp.ac.in\\nadmission@wilp.bits-pilani.ac.in\\n\", metadata={'source': 'Bits Mtech.pdf', 'page': 26})]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader('Bits Mtech.pdf') # Loading PDF File to extract data.\n",
    "pages = loader.load()\n",
    "pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7117a75d",
   "metadata": {},
   "source": [
    "## `Langchain with Gemini API`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9296012d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "31a20499",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_3 = ChatGoogleGenerativeAI(model = 'gemini-pro', google_api_key = GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7f6ffbf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Own\\.conda\\envs\\genai\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `BaseChatModel.predict` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"India, a vibrant symphony of ancient traditions and modern aspirations, is a land of unparalleled diversity. From the snow-capped Himalayas in the north to the sun-kissed beaches of the south, India boasts a mesmerizing tapestry of landscapes, cultures, and languages. Its ancient cities, adorned with architectural wonders like the Taj Mahal, stand alongside modern metropolises, pulsating with energy and innovation. India's vibrant spirituality, manifested in its countless temples and festivals, permeates every aspect of life, creating a unique and unforgettable experience for every visitor.\""
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_3.predict('Please write couple of lines of description on India. I want a complete paragraph')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2bf47522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Washington, D.C.'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_3.predict('What is the capital city of USA?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "80249181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It is difficult to predict the winner of the 2024 IPL Tournament as it is still a long way away and many factors can influence the outcome. However, based on the current form and squad strength of the teams, some potential contenders for the title include:\\n\\n* **Mumbai Indians:** The most successful team in IPL history, with five titles. They have a strong all-round squad with a good mix of experience and youth.\\n* **Chennai Super Kings:** Another strong contender, with four titles to their name. They have a consistent team with a good balance of batting, bowling, and fielding.\\n* **Royal Challengers Bangalore:** A team that has yet to win an IPL title, but always seems to be in contention. They have a star-studded batting lineup led by Virat Kohli.\\n* **Delhi Capitals:** A rising team that has made the playoffs in recent years. They have a strong bowling attack and a good all-round squad.\\n* **Lucknow Super Giants:** A new team that made a strong impression in their debut season in 2022. They have a good mix of experienced and young players.\\n\\nOther teams that could be in contention include the Kolkata Knight Riders, Punjab Kings, Sunrisers Hyderabad, and Rajasthan Royals. The tournament will be played in India from March to May 2024.'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_3.predict('Who will win 2024 IPL Tournament?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "04c0d6a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The outcome of any sporting event, including the IPL, is difficult to predict with certainty. However, here are some factors that could influence RCB's chances of winning the IPL Trophy:\\n\\n**Team Composition:**\\n* RCB has a strong batting lineup with Virat Kohli, Glenn Maxwell, and Faf du Plessis.\\n* Their bowling attack includes experienced players like Harshal Patel, Josh Hazlewood, and Mohammed Siraj.\\n\\n**Team Performance:**\\n* RCB has a consistent record in the IPL, reaching the playoffs in recent seasons.\\n* They have shown good form in the ongoing season, winning their first three matches.\\n\\n**Leadership and Coaching:**\\n* Faf du Plessis is an experienced captain who can guide the team effectively.\\n* Mike Hesson, the head coach, has a proven track record of success.\\n\\n**Schedule and Conditions:**\\n* RCB's schedule includes matches against strong teams like Mumbai Indians and Chennai Super Kings.\\n* The conditions in the UAE, where the tournament is being held, may favor spinners, which could be an advantage for RCB.\\n\\n**Injuries and Availability:**\\n* Injuries or unavailability of key players can significantly impact a team's performance.\\n* RCB has a relatively healthy squad at the moment.\\n\\n**Luck and Momentum:**\\n* Luck and momentum can play a role in determining the outcome of matches.\\n* RCB will need to be on the right side of both to win the trophy.\\n\\nBased on these factors, it is possible that RCB could be a contender for the IPL Trophy. However, it's important to note that there are many other strong teams in the competition, and the eventual winner will depend on several factors throughout the tournament.\""
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_3.predict('Will RCB win the IPL Trophy?')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (genai)",
   "language": "python",
   "name": "genai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
