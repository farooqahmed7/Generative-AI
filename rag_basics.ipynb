{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Qx5n-NAK8NF0"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_N1M0V28lmZ"
      },
      "source": [
        "- **`Ingestion`**\n",
        "- **`Retrieval`**\n",
        "- **`Generation`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dIeotq48miF",
        "outputId": "4b0e2679-972b-4d0e-cd0d-63d4ed605c2f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['The sun sets in the west, casting a warm glow over the horizon.',\n",
              " 'Birds chirp melodiously at dawn, welcoming the new day.',\n",
              " 'A gentle breeze rustles the leaves, creating a soothing sound.',\n",
              " 'The aroma of freshly brewed coffee fills the kitchen every morning.',\n",
              " 'Raindrops patter softly against the window, a comforting lullaby.',\n",
              " 'Children laugh and play in the park, their joy infectious.',\n",
              " 'The distant hum of traffic blends into the background noise of the city.',\n",
              " 'A cat purrs contentedly while curled up on the couch.',\n",
              " 'The smell of rain on dry earth is refreshing and nostalgic.',\n",
              " 'A colorful butterfly flutters gracefully from flower to flower.']"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "corpus_of_documents = [\"The sun sets in the west, casting a warm glow over the horizon.\",\n",
        "  \"Birds chirp melodiously at dawn, welcoming the new day.\",\n",
        "  \"A gentle breeze rustles the leaves, creating a soothing sound.\",\n",
        "  \"The aroma of freshly brewed coffee fills the kitchen every morning.\",\n",
        "  \"Raindrops patter softly against the window, a comforting lullaby.\",\n",
        "  \"Children laugh and play in the park, their joy infectious.\",\n",
        "  \"The distant hum of traffic blends into the background noise of the city.\",\n",
        "  \"A cat purrs contentedly while curled up on the couch.\",\n",
        "  \"The smell of rain on dry earth is refreshing and nostalgic.\",\n",
        "  \"A colorful butterfly flutters gracefully from flower to flower.\"]\n",
        "corpus_of_documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPEEXyYx_Nx1"
      },
      "source": [
        "### `Embeddings`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Fwa2zG1y8mkn"
      },
      "outputs": [],
      "source": [
        "# Using simple frequency based method.\n",
        "user_query = \"I am an Indian and I live in India\"\n",
        "document = \"India is a country for Indians and for everyone\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zkbXNFxn_k1R"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5yLMFk_NVbf",
        "outputId": "35a1dcfc-6bf6-4929-8d68-787e3bfb6982"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['i', 'am', 'an', 'indian', 'and', 'i', 'live', 'in', 'india']"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Query Tokens.\n",
        "query_tokens = user_query.lower().split(\" \")\n",
        "query_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qwIqjiqNcJH",
        "outputId": "919126a4-8f56-48db-f25a-d035ea14550e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['india', 'is', 'a', 'country', 'for', 'indians', 'and', 'for', 'everyone']"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Document Tokens.\n",
        "document_tokens = document.lower().split(\" \")\n",
        "document_tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GbH1LEwfN0xT",
        "outputId": "227e32c8-5b6e-4eaf-d4e2-5893488904f8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Counter({'i': 2,\n",
              "         'am': 1,\n",
              "         'an': 1,\n",
              "         'indian': 1,\n",
              "         'and': 1,\n",
              "         'live': 1,\n",
              "         'in': 1,\n",
              "         'india': 1})"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Creating Simple Embeddings.\n",
        "query_counter = Counter(query_tokens) # Frequency of Tokens.\n",
        "query_counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsukasG8OBPK",
        "outputId": "a9a8c46f-5c3d-4f7e-d4cb-f0d522d1c714"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Counter({'india': 1,\n",
              "         'is': 1,\n",
              "         'a': 1,\n",
              "         'country': 1,\n",
              "         'for': 2,\n",
              "         'indians': 1,\n",
              "         'and': 1,\n",
              "         'everyone': 1})"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "document_counter = Counter(document_tokens) # Frequency of Tokens.\n",
        "document_counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRlikBJaOiEw",
        "outputId": "03b73c4e-ae6b-44b3-9dd1-d751872f6c55"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['india', 'is', 'a', 'country', 'for', 'indians', 'and', 'everyone'])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "document_counter.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8f1P5DWQfV2",
        "outputId": "861efcea-0a7a-4fae-b249-569abdd0df65"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[2, 1, 1, 1, 1, 1, 1, 1]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embed = [] # Creating an empty list and storing in it. This will be numerical representation of the query.\n",
        "for i in query_counter.keys():\n",
        "  embed.append(query_counter[i])\n",
        " # print(query_counter[i])\n",
        "embed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POBUNmd2ROvc",
        "outputId": "7e6c0065-d7c6-440f-d203-e247247095e7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[1, 1, 1, 1, 2, 1, 1, 1]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embed2 = [] # Creating an empty list and storing in it. This will be numerical representation of the document.\n",
        "for i in document_counter.keys():\n",
        "  embed2.append(document_counter[i])\n",
        "embed2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECHyS2te3XFL"
      },
      "source": [
        "### `Similarity Scores Using Cosine Similarity`\n",
        "- Similarity Score will be calculated in between `User Query` and the available `Data`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCet3NGV2nqX",
        "outputId": "ff8459e1-52e7-4a51-830a-4c7907fbd2ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I am an Indian and I live in India\n",
            "India is a country for Indians and for everyone\n"
          ]
        }
      ],
      "source": [
        "print(user_query)\n",
        "print(document)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27Cl7qNpGnKW",
        "outputId": "b4581979-ebd8-4ef8-a801-1b0b6aa0e4a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "and\n",
            "india\n"
          ]
        }
      ],
      "source": [
        "# Similar words between user_query and the document.\n",
        "for tokens in query_counter.keys() & document_counter.keys():\n",
        "  print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "r8gxvsP6HRRB"
      },
      "outputs": [],
      "source": [
        "user_query2 = 'I am an Indian and I live in India and I love Indian Food'\n",
        "document2 = \"India is a country for Indians and for everyone and for those who loves Indian Food\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASFPt3swGpx0",
        "outputId": "3263ce6a-71c1-4729-86a6-23e52a80a103"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Counter({'i': 3,\n",
              "         'am': 1,\n",
              "         'an': 1,\n",
              "         'indian': 2,\n",
              "         'and': 2,\n",
              "         'live': 1,\n",
              "         'in': 1,\n",
              "         'india': 1,\n",
              "         'love': 1,\n",
              "         'food': 1})"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query_tokens2 = user_query2.lower().split(\" \")\n",
        "query_tokens2 = Counter(query_tokens2)\n",
        "query_tokens2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyVX8YElIRTZ",
        "outputId": "7b727f43-b3af-4915-a91f-812954b01cb0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Counter({'india': 1,\n",
              "         'is': 1,\n",
              "         'a': 1,\n",
              "         'country': 1,\n",
              "         'for': 3,\n",
              "         'indians': 1,\n",
              "         'and': 2,\n",
              "         'everyone': 1,\n",
              "         'those': 1,\n",
              "         'who': 1,\n",
              "         'loves': 1,\n",
              "         'indian': 1,\n",
              "         'food': 1})"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "document_tokens2 = document2.lower().split(\" \")\n",
        "document_tokens2 = Counter(document_tokens2)\n",
        "document_tokens2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3sZFYZXtIpgP",
        "outputId": "308fb299-a84a-4923-8e99-e8fee85db86a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "india\n",
            "and\n",
            "food\n",
            "indian\n"
          ]
        }
      ],
      "source": [
        "# Similar words between user_query and the document.\n",
        "for tokens in query_tokens2.keys() & document_tokens2.keys():\n",
        "  print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l69B675rKc5n",
        "outputId": "4943f531-1b27-477d-e9a8-b9a4a81adb95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Counter({'i': 2, 'am': 1, 'an': 1, 'indian': 1, 'and': 1, 'live': 1, 'in': 1, 'india': 1})\n",
            "****************************************************************************************************\n",
            "Counter({'for': 2, 'india': 1, 'is': 1, 'a': 1, 'country': 1, 'indians': 1, 'and': 1, 'everyone': 1})\n"
          ]
        }
      ],
      "source": [
        "print(query_counter)\n",
        "print('*'*100)\n",
        "print(document_counter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FY7TCYVJ7Er",
        "outputId": "b28a15d9-410a-49e6-c379-f68ecf6e126a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[1, 1]"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mylist = []\n",
        "for tokens in query_counter.keys() & document_counter.keys():\n",
        "  mylist.append(query_counter[tokens]*document_counter[tokens]) # Provides product of 2 common words i.e. 'and' & 'india' from two sentences.\n",
        "mylist # [1,1] means product of 2 common words.\n",
        "# {and:1, india:1 = [1,1]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jlu-0QzTPUl",
        "outputId": "c6b5728c-5d0b-4431-dae3-0ef564bee094"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dot_product = sum(mylist)\n",
        "dot_product"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-iYeoT0TkQJ",
        "outputId": "d6f4a406-4e9d-4063-e409-0a99a677d671"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3.3166247903554"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query_magnitude = math.sqrt(sum(query_counter[token] ** 2 for token in query_counter))\n",
        "query_magnitude"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBTPhn_HTkSi",
        "outputId": "f7886266-d6ab-4bfd-a8fa-a9b9af4274d6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3.3166247903554"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "document_magnitude = math.sqrt(sum(document_counter[token] ** 2 for token in document_counter))\n",
        "document_magnitude"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-xyki3WTkVF",
        "outputId": "b20dc06e-f996-4464-fcff-57f247f31208"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.18181818181818182"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Similarity Score between user_query and the document sentence.\n",
        "similarity = (dot_product)/(query_magnitude*document_magnitude)\n",
        "similarity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "vOYhbNKWH9xx"
      },
      "outputs": [],
      "source": [
        "# Similarity Score among the common words between user_query and the document sentences using Function.\n",
        "def cosine_similarity(query, document):\n",
        "  query_tokens = query.lower().split(\" \")\n",
        "  document_tokens = document.lower().split(\" \")\n",
        "\n",
        "  query_counter = Counter(query_tokens)\n",
        "  document_counter = Counter(document_tokens)\n",
        "\n",
        "  dot_product = sum(query_counter[token] * document_counter[token] for token in query_counter.keys() & document_counter.keys())\n",
        "\n",
        "  query_magnitude = math.sqrt(sum(query_counter[token] ** 2 for token in query_counter))\n",
        "  document_magnitude = math.sqrt(sum(document_counter[token] ** 2 for token in document_counter))\n",
        "  similarity = (dot_product)/(query_magnitude* document_magnitude) if query_magnitude * document_magnitude !=0 else 0\n",
        "  return similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0FkHp0VIijB",
        "outputId": "21446473-6ffa-4a30-d281-51d1132f7471"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.6324555320336759"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "user_query = 'Is farooq good data scientist and genai engineer?'\n",
        "document = 'Farooq is a genai engineer and he is very good in data science and machine learning'\n",
        "cosine_similarity(user_query,document)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSituJazIilc",
        "outputId": "c7492a67-0520-4129-a0e1-335daca61973"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.3651483716701107"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "user_query2 = 'Is farooq good with cooking skills?'\n",
        "document2 = 'Farooq is a genai engineer and he is very good in data science and machine learning'\n",
        "cosine_similarity(user_query2,document2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyKRNktgcJ44"
      },
      "source": [
        "### `LLM`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "KjhvU_uxdFs-"
      },
      "outputs": [],
      "source": [
        "#!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "3hGLj2vMdDAW"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "collapsed": true,
        "id": "uvDlz87NdKKk",
        "outputId": "8b1a62a0-766c-4d8e-ae17-b4c8e7e6b2f0"
      },
      "outputs": [
        {
          "ename": "RateLimitError",
          "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-42636699ded8>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOpenAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOPENAI_API_KEY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m completion = client.chat.completions.create(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'gpt-3.5-turbo'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     messages = [\n\u001b[1;32m      5\u001b[0m         \u001b[0;34m{\u001b[0m\u001b[0;34m'role'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m\"system\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"You are a poetic assistant, skilled in explaining complex programming concepts with creative approaches.\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    638\u001b[0m         \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeout\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mNotGiven\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNOT_GIVEN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 640\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    641\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1248\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m         )\n\u001b[0;32m-> 1250\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1252\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    929\u001b[0m         \u001b[0mstream_cls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_StreamT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m     ) -> ResponseT | _StreamT:\n\u001b[0;32m--> 931\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    932\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1013\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mretries\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m                 \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1015\u001b[0;31m                 return self._retry_request(\n\u001b[0m\u001b[1;32m   1016\u001b[0m                     \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m                     \u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1061\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m   1064\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1013\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mretries\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m                 \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1015\u001b[0;31m                 return self._retry_request(\n\u001b[0m\u001b[1;32m   1016\u001b[0m                     \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m                     \u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1061\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m   1064\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1030\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         return self._process_response(\n",
            "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
          ]
        }
      ],
      "source": [
        "client = OpenAI(api_key = OPENAI_API_KEY)\n",
        "completion = client.chat.completions.create(\n",
        "    model = 'gpt-3.5-turbo',\n",
        "    messages = [\n",
        "        {'role' : \"system\", \"content\": \"You are a poetic assistant, skilled in explaining complex programming concepts with creative approaches.\"},\n",
        "        {'role': \"system\", \"content\": \"Compose a poem that explains the concept of recursion in programing.\"}\n",
        "    ]\n",
        ")\n",
        "print(completion.choices[0].message)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "xGW4_ub1ehzw"
      },
      "outputs": [],
      "source": [
        "# Fetching the Sentence which is most relevant to the query and the corpus based on Cosine Similarity.\n",
        "# This is Part of 'RANKED RESULT'.\n",
        "def return_response(query, corpus):\n",
        "  similarities = []\n",
        "  for doc in corpus:\n",
        "    similarity = cosine_similarity(query, doc)\n",
        "    similarities.append(similarity)\n",
        "  return corpus_of_documents[similarities.index(max(similarities))] # Fetching the maximum similarity i.e. most relevant result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "wmZ4hnVdfGso"
      },
      "outputs": [],
      "source": [
        "corpus_of_documents = [\"The sun sets in the west, casting a warm glow over the horizon.\",\n",
        "  \"Birds chirp melodiously at dawn, welcoming the new day.\",\n",
        "  \"A gentle breeze rustles the leaves, creating a soothing sound.\",\n",
        "  \"The aroma of freshly brewed coffee fills the kitchen every morning.\",\n",
        "  \"Raindrops patter softly against the window, a comforting lullaby.\",\n",
        "  \"Children laugh and play in the park, their joy infectious.\",\n",
        "  \"The distant hum of traffic blends into the background noise of the city.\",\n",
        "  \"A cat purrs contentedly while curled up on the couch.\",\n",
        "  \"The smell of rain on dry earth is refreshing and nostalgic.\",\n",
        "  \"A colorful butterfly flutters gracefully from flower to flower.\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "qvxN2-EffvwQ"
      },
      "outputs": [],
      "source": [
        "# Ask query which has relevance to above corpus\n",
        "user_query = 'Is smell of rain will be refreshing?'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "2lS1N85wfiRL",
        "outputId": "0c14ad09-a62f-41bb-fbbb-dabd140e4fb6"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The smell of rain on dry earth is refreshing and nostalgic.'"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "return_response(user_query, corpus_of_documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "l2MZKxJSfyZX",
        "outputId": "c7254510-690b-442a-dad9-a6219e5fce8b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Children laugh and play in the park, their joy infectious.'"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "user_query = 'what Children do in the park?'\n",
        "return_response(user_query, corpus_of_documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "M0n0DWpNfyb_"
      },
      "outputs": [],
      "source": [
        "### 'GENERATION'\n",
        "full_response = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "iyoOHLqKmLPM"
      },
      "outputs": [],
      "source": [
        "user_input = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "YRWdDEawmD3U"
      },
      "outputs": [],
      "source": [
        "relevant_document = return_response(user_query, corpus_of_documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lT9PC05_jwKi",
        "outputId": "488b7d18-dc21-4bb5-dafc-bf8c5a2fced8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "This is the given information: Children laugh and play in the park, their joy infectious.\n",
            "The user input is: \n",
            "Compile the information to the user based on the given information and the user input.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "prompt = f\"\"\"\n",
        "This is the given information: {relevant_document}\n",
        "The user input is: {user_input}\n",
        "Compile the information to the user based on the given information and the user input.\n",
        "\"\"\"\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3g8MwBi5jwNA"
      },
      "outputs": [],
      "source": [
        "# Passing our Prompt to the OpenAI Model and generating the Output.\n",
        "client = OpenAI(api_key = OPENAI_API_KEY)\n",
        "completion = client.chat.completions.create(\n",
        "    model = 'gpt-3.5-turbo',\n",
        "    messages = [\n",
        "        {'role' : \"system\", \"content\": \"You are a friendly bot. You answer in very short sentences and do not include extra information.\"},\n",
        "        {'role': \"system\", \"content\": prompt}\n",
        "    ]\n",
        ")\n",
        "print(completion.choices[0].message)\n",
        "print(completion.choices[0].message.content)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
